---
title: "Chapitre 3 ‚Äî √âvaluation et Validation"
---

## üéØ Objectifs d'apprentissage

- **Comprendre** pourquoi on ne teste jamais sur les donn√©es d'entra√Ænement.
- **Ma√Ætriser** le protocole de validation (Train / Test Split).
- **Savoir lire** une Matrice de Confusion.
- **Calculer et interpr√©ter** les m√©triques cl√©s : Pr√©cision, Rappel, F1-Score.
- **Diagnostiquer** le sur-apprentissage (Overfitting) et le sous-apprentissage (Underfitting).

---

## 1. Le Protocole d'√âvaluation

Comment savoir si notre IA est "intelligente" ou si elle a juste appris par c≈ìur ?

### 1.1 L'analogie de l'examen

Imaginez un professeur qui donne 100 exercices √† ses √©l√®ves pour s'entra√Æner.
*   **Cas A** : L'examen final est compos√© de 10 exercices tir√©s *exactement* des 100 exercices d'entra√Ænement.
    *   *R√©sultat* : Un √©l√®ve qui a appris les r√©ponses par c≈ìur aura 20/20 sans rien comprendre.
*   **Cas B** : L'examen final est compos√© de 10 *nouveaux* exercices, jamais vus, mais du m√™me type.
    *   *R√©sultat* : Seul l'√©l√®ve qui a compris la logique r√©ussira.

En Machine Learning, on veut √©viter le Cas A. On veut tester la capacit√© de **g√©n√©ralisation**.

### 1.2 Train / Test Split

La r√®gle d'or est de diviser nos donn√©es disponibles en deux ensembles disjoints **avant** de commencer quoi que ce soit.

1.  **Jeu d'Entra√Ænement (Train Set)** : Environ **80%** des donn√©es. C'est le manuel scolaire. Le mod√®le l'utilise pour ajuster ses param√®tres (trouver la droite, les voisins, etc.).
2.  **Jeu de Test (Test Set)** : Environ **20%** des donn√©es. C'est l'examen final. On le cache au mod√®le pendant l'entra√Ænement. On ne l'utilise qu'√† la toute fin pour mesurer la performance.

> **Attention** : Il est strictement interdit d'entra√Æner le mod√®le sur le Test Set. Sinon, c'est de la triche (Data Leakage).

### 1.3 La Validation Crois√©e (Cross-Validation)

Le probl√®me du Train/Test Split simple, c'est que le r√©sultat peut d√©pendre du hasard du d√©coupage. Si vous avez de la chance, le Test Set est "facile". Si vous n'avez pas de chance, il est "difficile".

Pour avoir une estimation plus robuste, on utilise la **K-Fold Cross-Validation** :
1.  On coupe les donn√©es en **K** parts √©gales (ex: K=5).
2.  On entra√Æne sur 4 parts et on teste sur la 5√®me.
3.  On recommence en changeant la part de test.
4.  On fait √ßa 5 fois.
5.  On fait la **moyenne** des 5 scores obtenus.

C'est la m√©thode standard quand on a peu de donn√©es (< 10 000 exemples).

---

## 2. M√©triques de Classification

Dire "mon mod√®le a 90% de r√©ussite" (Accuracy) ne suffit pas, surtout si les classes sont d√©s√©quilibr√©es.
*Exemple : Dans un dataset de d√©tection de cancer o√π 99% des patients sont sains, un mod√®le qui dit "Tout le monde est sain" a 99% de r√©ussite... mais il est inutile.*

### 2.1 La Matrice de Confusion

C'est l'outil fondamental pour diss√©quer les erreurs. C'est un tableau crois√© entre la r√©alit√© et la pr√©diction.

Prenons un classifieur binaire (Chat vs Non-Chat) :

| | **Pr√©dit : CHAT** (Positif) | **Pr√©dit : NON-CHAT** (N√©gatif) |
| :--- | :---: | :---: |
| **Vrai : CHAT** | **Vrai Positif (VP)** <br> *(Bravo !)* | **Faux N√©gatif (FN)** <br> *(Loup√©, c'√©tait un chat)* |
| **Vrai : NON-CHAT** | **Faux Positif (FP)** <br> *(Fausse alerte)* | **Vrai N√©gatif (VN)** <br> *(Bravo !)* |

> **Exemple Num√©rique** :
> Sur 100 images (50 chats, 50 chiens) :
> *   **VP = 40** (40 chats bien reconnus)
> *   **FN = 10** (10 chats rat√©s, pris pour des chiens)
> *   **FP = 5** (5 chiens pris pour des chats)
> *   **VN = 45** (45 chiens bien reconnus)

### 2.2 Les M√©triques D√©riv√©es

√Ä partir de ces 4 nombres, on calcule des scores plus fins :

1.  **Pr√©cision (Precision)** : La qualit√© de la pr√©diction positive.
    *   *"Quand le mod√®le crie 'CHAT', a-t-il raison ?"*
    *   $$ \text{Pr√©cision} = \frac{VP}{VP + FP} $$
    *   *Important pour* : Filtre anti-spam (on ne veut pas mettre un mail important en spam).

2.  **Rappel (Recall / Sensibilit√©)** : La capacit√© √† trouver tous les positifs.
    *   *"Parmi tous les vrais chats qui existent, combien en a-t-il trouv√© ?"*
    *   $$ \text{Rappel} = \frac{VP}{VP + FN} $$
    *   *Important pour* : M√©decine (on ne veut rater aucun malade, quitte √† faire des fausses alertes).

3.  **F1-Score** : La moyenne harmonique des deux.
    *   C'est un bon r√©sum√© si on veut un √©quilibre entre Pr√©cision et Rappel.
    *   $$ F1 = 2 \times \frac{\text{Pr√©cision} \times \text{Rappel}}{\text{Pr√©cision} + \text{Rappel}} $$

### 2.3 Courbe ROC et AUC

Quand un mod√®le pr√©dit une probabilit√© (ex: "Il y a 70% de chances que ce soit un chat"), on doit choisir un **seuil** pour d√©cider (ex: si > 50% alors Chat).
Mais si on bouge ce seuil, la Pr√©cision et le Rappel changent !

La **Courbe ROC** (Receiver Operating Characteristic) trace le taux de Vrais Positifs contre le taux de Faux Positifs pour **tous les seuils possibles**.

*   **AUC (Area Under Curve)** : C'est l'aire sous cette courbe.
    *   **AUC = 0.5** : Le mod√®le tire √† pile ou face (nul).
    *   **AUC = 1.0** : Le mod√®le est parfait.
    *   C'est une excellente m√©trique pour comparer deux mod√®les ind√©pendamment du seuil choisi.

---

## 3. Le Fl√©au du Sur-apprentissage (Overfitting)

C'est le probl√®me n¬∞1 en Machine Learning.

### 3.1 D√©finition

L'**Overfitting**, c'est quand le mod√®le apprend "trop bien" les donn√©es d'entra√Ænement, y compris leur bruit et leurs anomalies, au point de perdre la vue d'ensemble.
Il devient excellent sur le Train Set, mais catastrophique sur le Test Set.

*   **Analogie** : Un costume taill√© sur mesure pour une personne pr√©cise n'ira √† personne d'autre. Un costume de pr√™t-√†-porter (plus g√©n√©ral) ira "√† peu pr√®s" √† tout le monde.

### 3.2 Le Sous-apprentissage (Underfitting)

C'est l'inverse. Le mod√®le est trop simple pour capturer la logique des donn√©es. Il est mauvais en Train ET en Test.
*   *Exemple* : Essayer de pr√©dire la bourse avec une simple ligne droite.

### 3.3 Le Compromis Biais-Variance

En statistique, l'erreur d'un mod√®le se d√©compose en trois termes (D√©composition Biais-Variance) :

$$ E[(y - \hat{f}(x))^2] = \text{Biais}^2 + \text{Variance} + \sigma^2 $$

1.  **Biais (Bias)** : Erreur due √† des hypoth√®ses trop simplistes (ex: croire que tout est une ligne droite). Un fort biais entra√Æne du **Sous-apprentissage**.
2.  **Variance** : Erreur due √† une trop grande sensibilit√© aux petites fluctuations du jeu d'entra√Ænement. Une forte variance entra√Æne du **Sur-apprentissage**.
3.  **Erreur Irr√©ductible ($\sigma^2$)** : Le bruit inh√©rent aux donn√©es, qu'aucun mod√®le ne peut pr√©dire.

On cherche le juste milieu (le "Sweet Spot") pour minimiser la somme Biais¬≤ + Variance.

| √âtat | Performance Train | Performance Test | Diagnostic | Solution |
| :--- | :--- | :--- | :--- | :--- |
| **Underfitting** | Mauvaise | Mauvaise | Mod√®le trop simple | Complexifier le mod√®le (plus de param√®tres). |
| **Bon Mod√®le** | Bonne | Bonne | √âquilibre trouv√© | - |
| **Overfitting** | Excellente | Mauvaise | Mod√®le trop complexe | Simplifier, ajouter des donn√©es, r√©gulariser. |

### 3.4 Visualisation Interactive

Jouez avec le **degr√© du polyn√¥me** (la complexit√© du mod√®le).
*   **Degr√© 1 (Ligne droite)** : Underfitting. Le mod√®le est trop rigide pour suivre la courbe verte.
*   **Degr√© 3-4** : Bon mod√®le. Il capture la forme g√©n√©rale.
*   **Degr√© 10+** : Overfitting. Le mod√®le (rouge) passe par tous les points bleus (Train Error $\approx$ 0) mais fait n'importe quoi entre les deux (Test Error explose).

<iframe class="embedded-notebook" src="/observables/overfitting/" width="100%" height="800" frameborder="0" style="border: 1px solid #eee; border-radius: 8px;"></iframe>

---

**Prochain chapitre** : [Chapitre 4 ‚Äî Apprentissage Non Supervis√©](/cours/CM4/)
