---
title: "Chapitre 1 ‚Äî Introduction et Repr√©sentation des donn√©es"
---

## üéØ Objectifs d'apprentissage

- **Comprendre** ce qu'est l'IA, son histoire et ses grandes familles (Symbolique vs Connexionniste).
- **Savoir num√©riser** le monde r√©el : comment transformer une image, un texte ou un son en un vecteur de nombres.
- **Ma√Ætriser** la notion de dimension et d'espace vectoriel.
- **Calculer** une distance entre deux donn√©es pour √©valuer leur similarit√©.
- **Comprendre** l'importance cruciale de la normalisation des donn√©es.

---

## 1. Qu'est-ce que l'IA ?

### 1.1 D√©finition et Nuances

L'Intelligence Artificielle (IA) est un domaine vaste et parfois mal d√©fini. Une d√©finition pragmatique serait :
> "L'ensemble des th√©ories et des techniques mises en ≈ìuvre en vue de r√©aliser des machines capables de simuler l'intelligence."

Il faut distinguer deux concepts souvent confondus :
*   **IA Faible (Weak AI)** : Une machine capable de r√©soudre **un** probl√®me sp√©cifique mieux qu'un humain (ex: jouer aux √©checs, reconna√Ætre un cancer sur une radio). C'est l'IA d'aujourd'hui.
*   **IA Forte (Strong AI / AGI)** : Une machine dot√©e de conscience, capable d'apprendre n'importe quelle t√¢che et de raisonner comme un humain. C'est l'IA de la science-fiction (pour l'instant).

### 1.2 Une br√®ve histoire de l'IA

L'IA n'est pas n√©e avec ChatGPT. C'est une discipline qui a connu des cycles d'euphorie ("Printemps de l'IA") et de d√©sillusion ("Hivers de l'IA").

*   **1950 ‚Äî Le Test de Turing** : Alan Turing propose un test : si un humain ne peut pas distinguer une machine d'un autre humain lors d'une conversation textuelle, la machine est "intelligente".
*   **1956 ‚Äî Conf√©rence de Dartmouth** : Naissance officielle du terme "Intelligence Artificielle".
*   **1950-1980 ‚Äî L'√Çge d'Or de l'IA Symbolique** : On pensait pouvoir tout r√©soudre avec de la logique formelle ("Si... Alors...").
    *   *Succ√®s* : Les syst√®mes experts (m√©decine, √©checs).
    *   *√âchec* : Incapacit√© √† g√©rer le flou, l'ambigu√Øt√© du langage ou la vision (reconna√Ætre un chat est impossible avec des r√®gles "Si... Alors...").
*   **1990-2010 ‚Äî L'√àre du Machine Learning (Statistique)** : Changement de paradigme. On arr√™te de dicter les r√®gles √† la machine, on lui donne des **donn√©es** pour qu'elle trouve les r√®gles elle-m√™me.
*   **2012 ‚Äî La R√©volution Deep Learning** : Le retour des r√©seaux de neurones (invent√©s bien plus t√¥t) gr√¢ce √† la puissance de calcul (cartes graphiques GPU) et aux m√©gadonn√©es (Big Data). AlexNet √©crase la concurrence en reconnaissance d'image.
*   **2022 ‚Äî L'√àre G√©n√©rative** : ChatGPT, Midjourney. L'IA ne se contente plus de classer, elle cr√©e.

### 1.3 Les trois paradigmes en r√©sum√©

| Paradigme | P√©riode | Principe | Exemple |
| :--- | :--- | :--- | :--- |
| **Symbolique** | 1950-1990 | R√®gles logiques √©crites par des humains. | "Si fi√®vre > 38¬∞C et toux, alors grippe." |
| **Machine Learning** | 1990-2010 | Algorithmes statistiques apprenant sur des donn√©es structur√©es. | Pr√©dire le prix d'une maison selon sa surface. |
| **Deep Learning** | 2010-... | R√©seaux de neurones profonds apprenant des repr√©sentations complexes. | Reconna√Ætre un visage, traduire un texte. |

---

## 2. Repr√©sentation des donn√©es : Tout est vecteur

Pour qu'un ordinateur puisse traiter une information (une image, une phrase, un son), cette information doit √™tre convertie en une liste de nombres. C'est l'√©tape de **num√©risation** ou d'**encodage**.

En math√©matiques, une liste ordonn√©e de nombres s'appelle un **vecteur**.

### 2.1 Donn√©es Tabulaires (Structur√©es)

C'est le cas le plus simple. Imaginez un fichier Excel d√©crivant des appartements. Chaque ligne est un **exemple** (ou *sample*), chaque colonne est une **caract√©ristique** (ou *feature*).

| Surface ($m^2$) | Pi√®ces | √âtage | Prix (‚Ç¨) |
| :---: | :---: | :---: | :---: |
| 45 | 2 | 1 | 200 000 |
| 80 | 4 | 3 | 350 000 |

L'appartement n¬∞1 est repr√©sent√© par le vecteur $\mathbf{x}^{(1)}$ :
$$ \mathbf{x}^{(1)} = \begin{pmatrix} 45 \\ 2 \\ 1 \end{pmatrix} $$

Ici, notre espace est de **dimension 3** ($d=3$). Chaque appartement est un point dans un espace 3D.

### 2.2 Images (Non structur√©es)

Une image num√©rique est une grille de pixels.
*   **Noir et Blanc** : Chaque pixel est un nombre entre 0 (noir) et 255 (blanc).
    *   Une image de $28 \times 28$ pixels contient $28 \times 28 = 784$ nombres.
    *   Pour l'IA, on "aplatit" cette grille pour en faire un vecteur g√©ant de dimension 784.
*   **Couleur (RGB)** : Chaque pixel a 3 valeurs (Rouge, Vert, Bleu).
    *   Une image $28 \times 28$ couleur est un vecteur de dimension $28 \times 28 \times 3 = 2352$.

> **Intuition** : Pour l'ordinateur, une photo de chat n'est pas une "image", c'est une liste de 2352 nombres. Si on change un pixel, on d√©place l√©g√®rement le point dans cet espace gigantesque.

### 2.3 Texte (NLP)

Comment transformer "Le chat mange" en nombres ?
Une m√©thode simple est le **Bag of Words** (Sac de mots) :
1.  On d√©finit un vocabulaire (ex: 10 000 mots).
2.  On compte la pr√©sence de chaque mot.

Phrase : "Le chat mange le poisson"
Vecteur : `[chat: 1, chien: 0, le: 2, mange: 1, poisson: 1, ...]`

C'est un vecteur tr√®s **creux** (beaucoup de z√©ros) et de tr√®s grande dimension.

---

## 3. Notion de Distance et Similarit√©

Une fois que nos donn√©es sont des points dans un espace vectoriel, on peut mesurer √† quel point elles sont proches.
**En IA, la proximit√© g√©om√©trique signifie souvent une similarit√© s√©mantique.**
*   Deux appartements proches dans l'espace vectoriel ont des prix similaires.
*   Deux images proches (pixel par pixel) se ressemblent visuellement.

<iframe class="embedded-notebook" src="/observables/distances/index.html" width="100%" height="750px" style="border:none;"></iframe>

### 3.1 La Distance Euclidienne ($L_2$)

C'est la distance "√† vol d'oiseau", celle que vous mesurez avec une r√®gle. Elle d√©coule du th√©or√®me de Pythagore.

Pour deux points $A$ et $B$ en 2 dimensions :
$$ d(A, B) = \sqrt{(x_B - x_A)^2 + (y_B - y_A)^2} $$

**G√©n√©ralisation en dimension $d$** :
Pour deux vecteurs $\mathbf{u} = (u_1, ..., u_d)$ et $\mathbf{v} = (v_1, ..., v_d)$ :

$$ d(\mathbf{u}, \mathbf{v}) = \sqrt{\sum_{i=1}^{d} (u_i - v_i)^2} $$

> **Exercice mental** :
> *   Appartement A : 30m¬≤, 1 pi√®ce. Vecteur $\mathbf{a} = (30, 1)$
> *   Appartement B : 32m¬≤, 2 pi√®ces. Vecteur $\mathbf{b} = (32, 2)$
>
> Distance au carr√© : $(32-30)^2 + (2-1)^2 = 2^2 + 1^2 = 4 + 1 = 5$.
> Distance : $\sqrt{5} \approx 2.23$.

### 3.2 La Distance de Manhattan ($L_1$)

C'est la distance "taxi". Dans une ville quadrill√©e comme New York, on ne peut pas traverser les immeubles. On doit longer les rues.
$$ d(\mathbf{u}, \mathbf{v}) = \sum_{i=1}^{d} |u_i - v_i| $$
Dans l'exemple pr√©c√©dent : $|32-30| + |2-1| = 2 + 1 = 3$.

### 3.3 La Similarit√© Cosinus

Parfois, la magnitude (la longueur) du vecteur n'importe pas, seule sa **direction** compte.
*Exemple : Analyse de texte.*
*   Texte A : "Le chat mange." (Vecteur : [1, 1, 1])
*   Texte B : "Le chat mange. Le chat mange." (Vecteur : [2, 2, 2])

Ces deux textes ont le m√™me sens, mais le vecteur B est deux fois plus long. La distance Euclidienne serait grande.
La **Similarit√© Cosinus** mesure l'angle $\theta$ entre les deux vecteurs.

$$ \text{Cosinus}(\mathbf{u}, \mathbf{v}) = \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{u}\| \|\mathbf{v}\|} $$

*   Si $\cos(\theta) = 1$ : Angle de $0$ rad (Vecteurs colin√©aires, sens identique).
*   Si $\cos(\theta) = 0$ : Angle de $\frac{\pi}{2}$ rad (Vecteurs orthogonaux, rien √† voir).
*   Si $\cos(\theta) = -1$ : Angle de $\pi$ rad (Vecteurs oppos√©s).

---

## 4. L'importance cruciale de la Normalisation

Regardons notre exemple immobilier :
*   Surface : varie de 10 √† 200 (m¬≤).
*   Nombre de pi√®ces : varie de 1 √† 5.

Si on calcule la distance entre deux appartements :
*   Diff√©rence de surface : 10 m¬≤ $\rightarrow$ contribution de $10^2 = 100$ √† la distance.
*   Diff√©rence de pi√®ces : 2 pi√®ces $\rightarrow$ contribution de $2^2 = 4$ √† la distance.

**Probl√®me** : La variable "Surface" √©crase compl√®tement la variable "Pi√®ces" juste parce que ses chiffres sont plus grands. L'algorithme va penser que le nombre de pi√®ces n'a aucune importance !

### 4.1 La solution : Mise √† l'√©chelle (Scaling)

Il faut ramener toutes les variables sur une √©chelle comparable. Il existe deux m√©thodes principales :

#### A. Normalisation Min-Max
On ram√®ne tout entre 0 et 1. C'est simple mais sensible aux valeurs extr√™mes (outliers).
$$ x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}} $$

#### B. Standardisation (Z-Score)
C'est la m√©thode pr√©f√©r√©e des statisticiens. On centre la distribution sur 0 et on r√©duit l'√©cart-type √† 1.
$$ x_{std} = \frac{x - \mu}{\sigma} $$
*   $\mu$ (mu) : la moyenne.
*   $\sigma$ (sigma) : l'√©cart-type.

> **Note** : Si vos donn√©es suivent une loi Normale (Gaussienne), la Standardisation est bien plus robuste que le Min-Max.

**Exemple Min-Max** :
*   Surface min = 20, max = 120.
*   Mon appart fait 70m¬≤.
*   $x_{norm} = \frac{70 - 20}{120 - 20} = \frac{50}{100} = 0.5$.

Maintenant, la surface vaut 0.5 et le nombre de pi√®ces (s'il est aussi normalis√©) vaudra peut-√™tre 0.4. Les deux variables ont d√©sormais le m√™me "poids" dans le calcul de distance.

---

## 5. R√©sum√© du cours

1.  **L'IA** a √©volu√© des r√®gles logiques (Symbolique) vers l'apprentissage par l'exemple (Machine Learning).
2.  **Tout est vecteur** : Pour traiter le monde r√©el, on le transforme en listes de nombres.
3.  **L'espace vectoriel** : Chaque donn√©e est un point. La dimension de l'espace est le nombre de caract√©ristiques.
4.  **La distance** (Euclidienne) permet de mesurer la similarit√© entre deux donn√©es.
5.  **Normaliser** est obligatoire : Il ne faut jamais m√©langer des unit√©s diff√©rentes (m√®tres, kilos, euros) sans les mettre √† la m√™me √©chelle.

---

**Prochain chapitre** : [Chapitre 2 ‚Äî Apprentissage Supervis√© : R√©gression et KNN](/cours/CM2/)
