---
title: "Chapitre 2 ‚Äî Apprentissage Supervis√© : R√©gression, KNN et Arbres"
---

## üéØ Objectifs d'apprentissage

- **Comprendre** la diff√©rence fondamentale entre R√©gression et Classification.
- **Ma√Ætriser** le concept de R√©gression Lin√©aire Simple (droite des moindres carr√©s).
- **Comprendre** l'algorithme des K-Plus Proches Voisins (KNN) √©tape par √©tape.
- **D√©couvrir** les Arbres de D√©cision et leur logique de partitionnement.
- **Saisir** l'impact des hyperparam√®tres (comme K ou la profondeur de l'arbre).

---

## 1. L'Apprentissage Supervis√© : Le Paradigme Dominant

L'apprentissage supervis√© est aujourd'hui la forme d'IA la plus r√©pandue dans l'industrie.
Son principe est simple : **Apprendre par l'exemple**.

On fournit √† l'algorithme un jeu de donn√©es d'entra√Ænement (Dataset) contenant des couples :
$$ (\text{Entr√©e}, \text{Sortie Attendue}) $$

L'objectif de la machine est de trouver une fonction math√©matique $f$ telle que pour toute nouvelle entr√©e $x$, $f(x)$ soit une bonne pr√©diction de la sortie $y$.

### 1.1 Les deux grandes familles de probl√®mes

Tout d√©pend de la nature de la sortie $y$ (la "target") :

1.  **R√©gression** : La sortie est une valeur **continue** (un nombre).
    *   *Exemples* : Pr√©dire le prix d'une maison, la temp√©rature de demain, le chiffre d'affaires du mois prochain.
2.  **Classification** : La sortie est une valeur **discr√®te** (une cat√©gorie ou "classe").
    *   *Exemples* : Pr√©dire si un email est "Spam" ou "Non-Spam", si une image contient un "Chat" ou un "Chien".

---

## 2. La R√©gression Lin√©aire

C'est l'algorithme le plus simple et le plus ancien (Gauss, Legendre, d√©but XIXe si√®cle), mais il reste la base de tout.

### 2.1 L'intuition

Imaginez que vous avez des donn√©es sur la taille et le poids de plusieurs personnes. Si vous placez ces points sur un graphique, ils forment un nuage allong√©.
La r√©gression lin√©aire consiste √† tracer **la droite qui passe "au milieu" du nuage**, celle qui r√©sume le mieux la tendance g√©n√©rale.

### 2.2 Le Mod√®le Math√©matique

Pour une seule variable d'entr√©e $x$ (ex: taille), on cherche une droite d'√©quation :
$$ y = ax + b $$
*   $a$ (ou $w$) : La **pente** (poids/weight). Elle indique √† quel point $y$ augmente quand $x$ augmente.
*   $b$ : L'**ordonn√©e √† l'origine** (biais/bias). C'est la valeur de base quand $x=0$.

### 2.3 Visualisation Interactive

Essayez de placer des points pour voir comment la droite s'ajuste automatiquement pour minimiser l'erreur (MSE).
Remarquez comment un seul point aberrant (outlier) peut "tirer" la droite vers lui.

<iframe class="embedded-notebook" src="/observables/linear-regression/" width="100%" height="800" frameborder="0" style="border: 1px solid #eee; border-radius: 8px;"></iframe>

### 2.4 Minimiser l'erreur

On cherche $a$ et $b$ qui minimisent la somme des carr√©s des erreurs (Moindres Carr√©s) :

$$ MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 $$
*   $y_i$ : La vraie valeur.
*   $\hat{y}_i$ : La valeur pr√©dite par la droite ($ax_i + b$).

> **Note** : C'est pour cela qu'on parle de la m√©thode des "Moindres Carr√©s".

### 2.5 Comment trouver a et b ? (Descente de Gradient)

Pour trouver la meilleure droite, on ne teste pas toutes les combinaisons au hasard. On utilise la **Descente de Gradient**.
Imaginez que vous √™tes en haut d'une montagne (l'erreur est haute) et que vous voulez descendre dans la vall√©e (erreur minimale) les yeux band√©s.
1.  Vous t√¢tez le sol pour sentir la pente.
2.  Vous faites un petit pas dans le sens de la descente.
3.  Vous recommencez.

Math√©matiquement, on utilise la **d√©riv√©e**.
> **Intuition** : La d√©riv√©e nous donne la pente de la tangente. Si la pente est positive (√ßa monte), on doit aller √† gauche (diminuer $a$). Si la pente est n√©gative (√ßa descend), on doit aller √† droite (augmenter $a$).

On met √† jour les param√®tres petit √† petit dans la direction oppos√©e √† la pente.

> **Coin des Matheux : Solution Analytique vs It√©rative**
>
> Pour la R√©gression Lin√©aire, il existe en fait une formule exacte pour trouver le minimum directement, sans it√©rer. C'est l'**√âquation Normale** :
> $$ \theta = (X^T X)^{-1} X^T y $$
> Cela revient √† r√©soudre le syst√®me lin√©aire $\nabla E(\theta) = 0$.
>
> **Pourquoi utiliser la Descente de Gradient alors ?**
> *   L'inversion de matrice $(X^T X)^{-1}$ est tr√®s co√ªteuse en calcul ($O(n^3)$) si on a des millions de donn√©es.
> *   Pour les r√©seaux de neurones (non-lin√©aires), il n'existe **pas** de solution analytique ferm√©e. La Descente de Gradient est la seule m√©thode universelle.

---

## 3. K-Nearest Neighbors (KNN) : La Classification Intuitive

Le KNN (K-Plus Proches Voisins) est un algorithme de classification (il peut aussi faire de la r√©gression) qui n'a pas besoin d'entra√Ænement complexe. Son principe est sociologique :
> "Dis-moi qui sont tes voisins, je te dirai qui tu es."

### 3.1 L'Algorithme pas √† pas

Imaginons qu'on veuille classer un nouveau point myst√®re (un fruit inconnu) en "Pomme" ou "Poire" selon son poids et sa couleur.

1.  **Stocker** : On garde en m√©moire tous les exemples connus (le dataset d'entra√Ænement).
2.  **Mesurer** : Quand le nouveau point arrive, on calcule la **distance** (Euclidienne, voir CM1) entre ce point et *tous* les autres points connus.
3.  **Trier** : On garde les **K** points les plus proches (les voisins).
4.  **Voter** : On regarde la classe majoritaire parmi ces K voisins.
5.  **D√©cider** : On attribue cette classe au nouveau point.

### 3.2 Exemple Concret

On a 5 points connus et on cherche √† classer le point $X$ (le rond noir).
Prenons $K=3$.

*   Voisin 1 : Bleu (Distance 1.2)
*   Voisin 2 : Bleu (Distance 1.5)
*   Voisin 3 : Rouge (Distance 2.1)

**R√©sultat du vote** : 2 Bleus contre 1 Rouge.
**D√©cision** : Le point $X$ est class√© **Bleu**.

### 3.3 L'Hyperparam√®tre K

Le choix de **K** est crucial. C'est un "hyperparam√®tre" (un r√©glage que l'humain doit choisir avant de lancer l'algo).

*   **Si K = 1** : On copie simplement le voisin le plus proche.
    *   *Risque* : Tr√®s sensible au bruit. Si le voisin le plus proche est une erreur (un point rouge √©gar√© chez les bleus), on se trompe. C'est du **Sur-apprentissage (Overfitting)**.
*   **Si K est tr√®s grand** (ex: K = 100) : On regarde tr√®s loin.
    *   *Risque* : On finit par toujours pr√©dire la classe majoritaire globale, on perd les d√©tails locaux. C'est du **Sous-apprentissage (Underfitting)**.

> **R√®gle empirique** : On choisit souvent un K impair (3, 5, 7) pour √©viter les √©galit√©s lors du vote (ex: 2 Bleus vs 2 Rouges).

### 3.6 Variante : KNN Pond√©r√© (Weighted KNN)

Dans le KNN classique, tous les voisins ont le m√™me poids de vote.
*   Probl√®me : Un voisin tr√®s proche compte autant qu'un voisin √† la limite de la zone.
*   Solution : Donner plus de poids aux voisins proches. Souvent, le poids est l'inverse de la distance ($1/d$). Ainsi, un voisin coll√© au point myst√®re aura un vote d√©cisif.

### 3.4 Avantages et Inconv√©nients

| Avantages | Inconv√©nients |
| :--- | :--- |
| Tr√®s simple √† comprendre et √† coder. | **Lent** si beaucoup de donn√©es (doit calculer toutes les distances √† chaque fois). |
| Pas d'entra√Ænement (Lazy Learning). | Sensible aux donn√©es non normalis√©es (voir Chapitre 1). |
| Marche bien pour des fronti√®res complexes. | Souffre de la "Mal√©diction de la dimension". |

> **La Mal√©diction de la Dimension (Curse of Dimensionality)** :
> Plus on ajoute de variables (colonnes), plus l'espace devient "vaste" et vide. En tr√®s haute dimension (ex: 1000 variables), tous les points deviennent tr√®s √©loign√©s les uns des autres. La notion de "voisin proche" perd de son sens, car tout le monde est loin de tout le monde !

### 3.5 Visualisation Interactive

<iframe class="embedded-notebook" src="/observables/knn/" width="100%" height="800" frameborder="0"></iframe>

---

## 4. Les Arbres de D√©cision

Contrairement au KNN qui est bas√© sur la distance, les Arbres de D√©cision sont bas√©s sur des **r√®gles logiques**. C'est l'algorithme qui ressemble le plus au raisonnement humain.

### 4.1 Le Principe : "Diviser pour mieux r√©gner"

L'id√©e est de d√©couper l'espace des donn√©es en rectangles de plus en plus petits, en posant une s√©rie de questions binaires (Oui/Non).

*Exemple : Pr√©dire si un client va acheter un produit.*
1.  **Question 1** : A-t-il moins de 30 ans ?
    *   *Si OUI* : **Question 2** : Est-il √©tudiant ?
        *   *Si OUI* : **ACH√àTE**
        *   *Si NON* : **N'ACH√àTE PAS**
    *   *Si NON* : **Question 3** : A-t-il un bon salaire ?
        *   ...

### 4.2 Construction de l'arbre (Algorithme CART)

Comment l'ordinateur choisit-il les questions ? Il cherche la question qui s√©pare le mieux les classes, c'est-√†-dire qui maximise la **puret√©** des groupes cr√©√©s.

#### A. L'Indice de Gini (Impuret√©)

C'est la mesure standard utilis√©e par l'algorithme CART (Classification And Regression Trees).
Pour un groupe contenant des √©l√©ments de $C$ classes, l'indice de Gini est :

$$ G = 1 - \sum_{i=1}^{C} p_i^2 $$

O√π $p_i$ est la proportion d'√©l√©ments de la classe $i$ dans le groupe.

*   **Gini = 0** : Le groupe est pur (ex: 100% de Bleus). $1 - (1^2 + 0^2) = 0$.
*   **Gini = 0.5** : Le groupe est parfaitement m√©lang√© (50% Bleus, 50% Rouges). $1 - (0.5^2 + 0.5^2) = 1 - 0.5 = 0.5$.

#### B. L'Entropie de Shannon (D√©sordre)

Une autre mesure tr√®s utilis√©e (notamment dans l'algorithme C4.5) est l'**Entropie**, issue de la th√©orie de l'information.

$$ H = - \sum_{i=1}^{C} p_i \log_2(p_i) $$

*   **Entropie = 0** : Ordre parfait (groupe pur).
*   **Entropie = 1** : D√©sordre maximal (m√©lange 50/50).

#### C. Le Gain d'Information

Pour choisir la meilleure question (le meilleur "split"), l'algorithme calcule le **Gain d'Information** : c'est la diff√©rence entre l'impuret√© avant la coupure et l'impuret√© moyenne apr√®s la coupure.

$$ \text{Gain} = \text{Impuret√©}_{\text{Parent}} - \sum \frac{N_{\text{Enfant}}}{N_{\text{Total}}} \times \text{Impuret√©}_{\text{Enfant}} $$

On choisit la coupure qui maximise ce gain (c'est-√†-dire qui r√©duit le plus le d√©sordre).

### 4.3 Visualisation Interactive

Observez comment l'arbre d√©coupe l'espace.
*   **Profondeur 1** : Une seule coupure (une ligne droite). C'est un "Decision Stump".
*   **Profondeur √©lev√©e** : L'arbre cr√©e des rectangles tr√®s fins pour isoler chaque point. Attention au sur-apprentissage !

<iframe class="embedded-notebook" src="/observables/decision-tree/" width="100%" height="800" frameborder="0" style="border: 1px solid #eee; border-radius: 8px;"></iframe>

### 4.4 Avantages et Inconv√©nients

| Avantages | Inconv√©nients |
| :--- | :--- |
| **Interpr√©table** (White Box) : on peut dessiner l'arbre et expliquer la d√©cision. | Tr√®s instable : changer un seul point peut changer tout l'arbre. |
| G√®re bien les donn√©es mixtes (num√©riques et cat√©gorielles). | Tendance facile au sur-apprentissage (si on ne limite pas la profondeur). |
| Pas besoin de normaliser les donn√©es. | Les fronti√®res sont toujours orthogonales aux axes (en escalier). |

> **Note** : Pour pallier l'instabilit√©, on utilise des **For√™ts Al√©atoires** (Random Forests), qui sont simplement une collection de centaines d'arbres qui votent ensemble.

---

**Prochain chapitre** : [Chapitre 3 ‚Äî √âvaluation et Validation](/cours/CM3/)
