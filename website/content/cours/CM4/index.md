---
title: "Chapitre 4 ‚Äî Apprentissage Non Supervis√©"
---

## üéØ Objectifs d'apprentissage

- **Comprendre** la diff√©rence avec le supervis√© : l'absence d'√©tiquettes.
- **Ma√Ætriser** l'algorithme de Clustering K-Means.
- **Savoir choisir** le nombre de clusters (m√©thode du coude).
- **Comprendre** le principe de la R√©duction de Dimension (PCA) et son utilit√©.

---

## 1. L'Apprentissage Non Supervis√© : Explorer l'Inconnu

Dans l'apprentissage supervis√© (Chapitre 2), on avait un "professeur" qui donnait la bonne r√©ponse ($y$).
Ici, **il n'y a pas de r√©ponse attendue**. On donne au mod√®le un tas de donn√©es brutes et on lui dit :
> "D√©brouille-toi pour trouver une structure, des motifs ou des groupes l√†-dedans."

C'est beaucoup plus difficile, mais c'est essentiel car la majorit√© des donn√©es dans le monde ne sont pas √©tiquet√©es.

### Applications types
1.  **Clustering (Regroupement)** : Segmenter ses clients en groupes de consommation (Marketing).
2.  **R√©duction de dimension** : Simplifier des donn√©es trop complexes pour les visualiser.
3.  **D√©tection d'anomalies** : Rep√©rer une fraude bancaire car elle ne ressemble pas aux autres transactions.

---

## 2. Le Clustering avec K-Means

C'est l'algorithme roi du clustering. Son but est de partitionner les donn√©es en **K** groupes (clusters) de sorte que les points d'un m√™me groupe soient proches les uns des autres.

### 2.1 L'Algorithme des Centres Mobiles

C'est un algorithme it√©ratif qui cherche √† minimiser l'**Inertie Intra-Classe** (la somme des carr√©s des distances entre chaque point et le centre de son cluster).

$$ J = \sum_{j=1}^{K} \sum_{x_i \in C_j} ||x_i - \mu_j||^2 $$

1.  **Initialisation** : On place **K** points au hasard dans l'espace. Ce sont nos "Centres" (ou Centro√Ødes) provisoires.
2.  **Affectation** : Pour chaque point de donn√©es, on regarde quel est le Centre le plus proche et on lui attribue sa couleur.
3.  **Mise √† jour** : On calcule la **moyenne** de la position de tous les points rouges. Le Centre rouge se d√©place vers cette moyenne (le centre de gravit√© du groupe). Idem pour les bleus, verts, etc.
4.  **R√©p√©tition** : On recommence les √©tapes 2 et 3 jusqu'√† ce que les centres ne bougent plus (convergence).

### 2.2 Visualisation Interactive

Exp√©rimentez par vous-m√™me !
1.  Choisissez le nombre de clusters **K**.
2.  Cliquez sur **"Initialiser"** pour placer les centres au hasard.
3.  Avancez pas √† pas avec **"√âtape +1"** ou lancez **"Auto"**.
4.  *Bonus* : Cliquez n'importe o√π pour ajouter des points en temps r√©el.

<iframe class="embedded-notebook" src="/observables/kmeans/" width="100%" height="800" frameborder="0" style="border: 1px solid #eee; border-radius: 8px;"></iframe>

### 2.3 Comment choisir K ? (La m√©thode du Coude)

L'algorithme a besoin qu'on lui dise combien de groupes chercher ($K$). Mais souvent, on ne le sait pas !

Pour choisir, on trace la courbe de l'**Inertie** (la somme des distances entre les points et leur centre) en fonction de $K$.
*   Plus $K$ augmente, plus l'inertie baisse (avec $K=N$, l'inertie est nulle, chaque point est son propre groupe).
*   On cherche le point d'inflexion, le **"Coude"** (Elbow), o√π le gain de performance commence √† devenir n√©gligeable par rapport √† la complexit√© ajout√©e.

> **Visualisation Mentale** :
> Imaginez un bras pli√©.
> *   L'√©paule (K=1) : Inertie tr√®s haute.
> *   Le coude (K=3) : L'inertie a beaucoup baiss√©, le bras change de direction.
> *   Le poignet (K=10) : L'inertie continue de baisser mais doucement.
>
> On choisit le **coude**.

---

## 3. Le Clustering Hi√©rarchique

Contrairement au K-Means o√π l'on doit choisir K √† l'avance, le Clustering Hi√©rarchique construit une structure d'arbre (un **Dendrogramme**) qui contient toutes les solutions possibles, de 1 √† N clusters.

### 3.1 L'Approche Agglom√©rative (Bottom-Up)

1.  Au d√©part, chaque point est un cluster √† lui tout seul.
2.  On cherche les deux clusters les plus proches et on les fusionne.
3.  On r√©p√®te jusqu'√† ce qu'il ne reste qu'un seul gros cluster contenant tout le monde.

### 3.2 Le Dendrogramme

C'est le graphique qui r√©sume l'histoire des fusions.
*   En coupant l'arbre en haut, on obtient 2 gros clusters.
*   En coupant plus bas, on obtient 5, 10, 20 petits clusters.

C'est tr√®s utile en biologie (phylog√©n√©tique) pour classer les esp√®ces animales.

---

## 4. La R√©duction de Dimension : PCA

Travailler avec des donn√©es en 2D ou 3D est facile. Mais en IA, on a souvent 1000 ou 10 000 dimensions (variables).
C'est la **Mal√©diction de la Dimension** (Curse of Dimensionality) : l'espace devient vide, les distances ne veulent plus rien dire.

L'Analyse en Composantes Principales (PCA) permet de "r√©sumer" ces donn√©es en gardant le maximum d'information.

### 4.1 L'Intuition de l'Ombre

Imaginez une th√©i√®re (objet 3D). Vous voulez la prendre en photo (projection 2D) pour qu'on la reconnaisse le mieux possible.
*   Si vous la prenez du dessus, on voit juste un rond (le couvercle). On a perdu l'info "anse" et "bec". C'est une mauvaise projection.
*   Si vous la prenez de profil, on voit bien sa forme √©tal√©e. C'est une bonne projection.

La PCA cherche math√©matiquement cet "angle de vue" optimal. Elle cherche les axes o√π les donn√©es sont les plus **√©tal√©es** (l√† o√π la **variance** est maximale).

### 4.2 Le Coin des Matheux : Diagonalisation

Pour ceux qui ont fait de l'alg√®bre lin√©aire, la PCA n'est rien d'autre qu'une **diagonalisation de la matrice de covariance**.
1.  On centre les donn√©es (moyenne nulle).
2.  On calcule la matrice de covariance $\Sigma = \frac{1}{N} X^T X$.
3.  On cherche ses **valeurs propres** ($\lambda$) et **vecteurs propres** ($v$) tels que $\Sigma v = \lambda v$.
4.  Les vecteurs propres sont les "Axes Principaux" (la direction de la th√©i√®re).
5.  Les valeurs propres indiquent la quantit√© d'information (variance) port√©e par chaque axe.
    *   La proportion de variance expliqu√©e par l'axe $k$ est $\frac{\lambda_k}{\sum \lambda_i}$.
6.  On garde les $k$ vecteurs associ√©s aux plus grandes valeurs propres.

C'est une application directe de la r√©duction d'endomorphisme !

### 4.3 √Ä quoi √ßa sert ?

1.  **Visualisation** : On ne peut pas dessiner un graphique en 100 dimensions. On utilise la PCA pour projeter les donn√©es en 2D ou 3D et voir s'il y a des groupes.
2.  **Compression** : On garde 95% de l'information avec 10 fois moins de stockage.
3.  **Pr√©-traitement** : Enlever le bruit et les variables inutiles avant de lancer un algorithme d'apprentissage (√ßa acc√©l√®re le calcul).

---

**Prochain chapitre** : [Chapitre 5 ‚Äî R√©seaux de Neurones (Perceptron & MLP)](/cours/CM5/)
