---
title: "Chapitre 7 ‚Äî √âthique, Limites et Soci√©t√©"
---

## üéØ Objectifs d'apprentissage

- **D√©velopper** un esprit critique face au "Hype" de l'IA.
- **Comprendre** les m√©canismes des biais algorithmiques et leurs cons√©quences sociales.
- **Conna√Ætre** les enjeux d'explicabilit√© (Black Box).
- **Appr√©hender** le cadre l√©gal (RGPD, AI Act) et l'impact environnemental.

---

## 1. Les Biais Algorithmiques

On entend souvent : *"L'ordinateur est neutre, c'est des maths."* **C'est faux.**
Une IA est le reflet des donn√©es sur lesquelles elle a √©t√© entra√Æn√©e. Si les donn√©es contiennent les pr√©jug√©s de la soci√©t√©, l'IA va les apprendre et les amplifier.

### 1.1 Exemples c√©l√®bres
*   **Recrutement (Amazon)** : Un algo entra√Æn√© sur 10 ans de CV (majoritairement masculins dans la tech) a appris √† p√©naliser le mot "femme" ou les noms d'√©coles f√©minines. Il a d√ª √™tre abandonn√©.
*   **Justice (COMPAS)** : Un logiciel am√©ricain pr√©disant la r√©cidive attribuait syst√©matiquement un risque plus √©lev√© aux personnes noires, √† dossier √©gal.
*   **Reconnaissance faciale** : Les mod√®les marchent souvent moins bien sur les peaux fonc√©es car les datasets d'entra√Ænement contiennent surtout des personnes blanches.

### 1.2 D'o√π vient le biais ?
1.  **Biais de donn√©es** : L'√©chantillon n'est pas repr√©sentatif de la population.
2.  **Biais historique** : La r√©alit√© elle-m√™me est biais√©e (ex: √©carts de salaire H/F), l'IA ne fait que le constater et le reproduire.

> [!info] Le Coin des Matheux : D√©finir l'√âquit√© (Fairness)
> Comment prouver math√©matiquement qu'un algorithme est "juste" ? Il existe plusieurs d√©finitions incompatibles entre elles.
>
> Soit $\hat{Y}$ la pr√©diction (ex: embauch√©), $Y$ la r√©alit√© (ex: comp√©tent), et $A$ un attribut prot√©g√© (ex: Genre, $A=0$ ou $A=1$).
>
> **1. Parit√© D√©mographique (Demographic Parity)**
> On exige que le taux de s√©lection soit le m√™me pour tous les groupes.
> $$ P(\hat{Y}=1 | A=0) = P(\hat{Y}=1 | A=1) $$
> *Probl√®me* : Si le groupe $A=1$ est r√©ellement plus comp√©tent en moyenne (biais historique), cette r√®gle force l'algo √† √™tre moins pr√©cis pour respecter l'√©galit√©.
>
> **2. √âgalit√© des Chances (Equalized Odds)**
> On exige que les taux d'erreur soient les m√™mes.
> $$ P(\hat{Y}=1 | Y=y, A=0) = P(\hat{Y}=1 | Y=y, A=1) $$
> Cela signifie que le taux de Vrais Positifs (et Faux Positifs) doit √™tre identique pour les hommes et les femmes. C'est souvent consid√©r√© comme plus "juste" m√©ritocratiquement.
>
> **3. Calibration (Predictive Parity)**
> On exige que la probabilit√© pr√©dite refl√®te la m√™me probabilit√© r√©elle pour tous les groupes.
> $$ P(Y=1 | \hat{Y}=s, A=0) = P(Y=1 | \hat{Y}=s, A=1) $$
> C'est souvent ce que cherchent les banques : si l'algo dit "risque 20%", cela doit signifier "20% de d√©faut" que l'on soit homme ou femme.
>
> **Th√©or√®me d'Impossibilit√© (Chouldechova, 2017)** : Il est math√©matiquement impossible de satisfaire les trois crit√®res en m√™me temps si les groupes de base n'ont pas le m√™me taux de succ√®s r√©el. Il faut faire un choix politique.

---

## 2. S√©curit√© et Robustesse (Attaques Adverses)

Les r√©seaux de neurones sont fragiles. On peut tromper une IA avec des **Exemples Adverses**.
Il suffit d'ajouter un bruit invisible √† l'≈ìil nu sur une photo de Panda pour que l'IA soit s√ªre √† 99% que c'est un Gibbon.

> **Visualisation** :
> $$ \text{Image Panda} + \epsilon \times \text{Bruit (Neige)} = \text{Image Panda (pour nous)} $$
> Mais pour l'IA :
> $$ \text{Panda (57\%)} + \text{Bruit} \rightarrow \text{Gibbon (99\%)} $$

Cela pose un √©norme probl√®me de s√©curit√© : imaginez un panneau "Stop" avec un autocollant sp√©cial qui le fait passer pour un panneau "Limitation 100" aux yeux d'une voiture autonome.

---

## 3. L'Explicabilit√© (XAI) et la "Bo√Æte Noire"

Les r√©seaux de neurones profonds (Deep Learning) sont des **Black Boxes**.
On sait ce qui rentre (Input) et ce qui sort (Output), mais les millions de calculs interm√©diaires sont illisibles pour un humain.

### Pourquoi est-ce grave ?
*   **Droit √† l'explication** : Si une banque vous refuse un pr√™t ou si une IA m√©dicale diagnostique un cancer, vous avez le droit de savoir **pourquoi**. "L'ordinateur a dit non" n'est pas acceptable juridiquement ni √©thiquement.
*   **D√©bogage** : Si on ne comprend pas comment l'IA d√©cide, on ne peut pas corriger ses erreurs (ex: une voiture autonome qui confond la lune et un feu orange).

---

## 4. Impact Environnemental

L'IA "virtuelle" a un co√ªt physique bien r√©el.

*   **Entra√Ænement** : Entra√Æner un mod√®le comme GPT-4 consomme autant d'√©lectricit√© qu'une petite ville pendant des mois.
*   **Inf√©rence** : Chaque requ√™te √† ChatGPT consomme de l'eau (pour refroidir les serveurs) et de l'√©lectricit√©. Une recherche Google "IA" consomme 10x √† 30x plus qu'une recherche classique.
*   **Mat√©riel** : La fabrication des GPU n√©cessite des terres rares et de l'eau.

---

## 5. Cadre L√©gal et R√©gulation

L'Europe est pionni√®re dans la r√©gulation de l'IA.

### 5.1 Le RGPD (2018)
L'article 22 prot√®ge les citoyens contre les d√©cisions **enti√®rement automatis√©es** ayant un effet juridique. Un humain doit toujours pouvoir intervenir ("Human in the loop").

### 5.2 L'AI Act (2024)
C'est la premi√®re loi globale sur l'IA. Elle classe les IA par niveau de risque (Pyramide des risques) :

<div style="display: flex; justify-content: center; margin: 20px 0;">
  <svg width="350" height="250" viewBox="0 0 350 250">
    <!-- Green -->
    <polygon points="20,240 330,240 280,180 70,180" fill="#4caf50" stroke="white" stroke-width="2"/>
    <text x="175" y="220" text-anchor="middle" fill="white" font-weight="bold">Risque Minimal (Libre)</text>

    <!-- Yellow -->
    <polygon points="70,180 280,180 240,120 110,120" fill="#ffeb3b" stroke="white" stroke-width="2"/>
    <text x="175" y="160" text-anchor="middle" fill="black" font-weight="bold">Risque Limit√© (Transparence)</text>

    <!-- Orange -->
    <polygon points="110,120 240,120 200,60 150,60" fill="#ff9800" stroke="white" stroke-width="2"/>
    <text x="175" y="100" text-anchor="middle" fill="white" font-weight="bold">Haut Risque (R√©gul√©)</text>

    <!-- Red -->
    <polygon points="150,60 200,60 175,10" fill="#f44336" stroke="white" stroke-width="2"/>
    <text x="175" y="45" text-anchor="middle" fill="white" font-weight="bold" font-size="10">Inacceptable (Interdit)</text>
  </svg>
</div>

1.  **Risque Inacceptable (Interdit)** :
    *   Notation sociale (Social Scoring).
    *   Manipulation subliminale.
    *   Reconnaissance faciale de masse en temps r√©el.
2.  **Haut Risque (R√©gul√©)** :
    *   Sant√©, √âducation, Recrutement, Justice, Transports.
    *   *Obligations* : Transparence, Qualit√© des donn√©es, Supervision humaine.
3.  **Risque Limit√© (Transparence)** :
    *   Chatbots (doivent dire qu'ils sont des robots).
    *   Deepfakes (doivent √™tre marqu√©s comme tels).
4.  **Risque Minimal (Libre)** :
    *   Jeux vid√©o, filtres anti-spam.

---

## 6. Avenir du Travail

L'IA ne va probablement pas "remplacer" les humains, mais **les humains qui utilisent l'IA vont remplacer ceux qui ne l'utilisent pas**.
On se dirige vers une collaboration Homme-Machine (IA Augment√©e) plut√¥t qu'un grand remplacement, m√™me si certains m√©tiers r√©p√©titifs ou bas√©s sur la synth√®se d'information seront fortement impact√©s.

---

**Fin du cours !**
