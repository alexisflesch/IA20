---
title: "TD 3 ‚Äî √âvaluation de Mod√®les"
---

## üéØ Objectifs

- Construire une Matrice de Confusion.
- Calculer Pr√©cision, Rappel, Accuracy, F1.
- Tracer une courbe ROC manuellement.
- Analyser des cas d'usage r√©els.

---

## Exercice 1 : La Matrice de Confusion

Vous avez entra√Æn√© un mod√®le pour d√©tecter des chats sur des photos.
Vous testez votre mod√®le sur 10 photos. Voici les r√©sultats :

| Photo | Vraie Classe | Pr√©diction du Mod√®le |
| :--- | :--- | :--- |
| 1 | Chat | Chat |
| 2 | Chat | Chien |
| 3 | Chien | Chien |
| 4 | Chien | Chat |
| 5 | Chat | Chat |
| 6 | Chat | Chat |
| 7 | Chien | Chien |
| 8 | Chien | Chien |
| 9 | Chat | Chien |
| 10 | Chien | Chien |

**Questions :**
1.  Combien y a-t-il de **Vrais Positifs (VP)** ? (Vrai = Chat, Pr√©dit = Chat)
2.  Combien y a-t-il de **Faux N√©gatifs (FN)** ? (Vrai = Chat, Pr√©dit = Chien)
3.  Combien y a-t-il de **Faux Positifs (FP)** ? (Vrai = Chien, Pr√©dit = Chat)
4.  Combien y a-t-il de **Vrais N√©gatifs (VN)** ? (Vrai = Chien, Pr√©dit = Chien)
5.  Dessinez la Matrice de Confusion.

---

## Exercice 2 : Calcul des M√©triques

√Ä partir de la matrice de l'exercice 1 :

**Questions :**
1.  Calculez l'**Accuracy** (Taux de r√©ussite global).
    *   Formule : $(VP + VN) / Total$
2.  Calculez la **Pr√©cision**.
    *   Formule : $VP / (VP + FP)$
    *   *Interpr√©tation* : Quand le mod√®le dit "C'est un chat", a-t-il souvent raison ?
3.  Calculez le **Rappel** (Recall).
    *   Formule : $VP / (VP + FN)$
    *   *Interpr√©tation* : A-t-il trouv√© tous les chats qui existaient ?
4.  Calculez le **F1-Score**.
    *   Formule : $2 \times \frac{P \times R}{P + R}$

---

## Exercice 3 : Courbe ROC et Seuil

Un mod√®le ne sort pas juste "Chat" ou "Chien", mais une probabilit√© d'√™tre un Chat.
Voici les probabilit√©s pr√©dites pour 5 images (les 3 premi√®res sont des Chats, les 2 derni√®res des Chiens).

*   Img 1 (Chat) : 0.9
*   Img 2 (Chat) : 0.6
*   Img 3 (Chat) : 0.4 (Erreur potentielle)
*   Img 4 (Chien) : 0.7 (Erreur potentielle)
*   Img 5 (Chien) : 0.1

On doit choisir un **seuil** de d√©cision. Si Proba > Seuil, on dit "Chat".

**Questions :**
1.  **Seuil = 0.5** : Quelles sont les pr√©dictions ? Calculez le Taux de Vrais Positifs (TPR = Rappel) et le Taux de Faux Positifs (FPR = FP / Total N√©gatifs).
    *   *Rappel* : TPR = VP / (VP+FN). FPR = FP / (FP+VN).
2.  **Seuil = 0.8** : Le mod√®le devient plus s√©v√®re. Recalculez TPR et FPR.
3.  **Seuil = 0.2** : Le mod√®le devient plus laxiste. Recalculez TPR et FPR.
4.  Placez ces 3 points sur un graphique (Axe X = FPR, Axe Y = TPR). Reliez-les pour esquisser la courbe ROC.
5.  L'AUC (Aire sous la courbe) est-elle proche de 1 ou de 0.5 ?

---

## Exercice 4 : Analyse de Sc√©narios

Dans la vraie vie, on ne cherche pas toujours √† maximiser l'Accuracy.

**Cas A : D√©tection de fraude bancaire**
*   La fraude est rare (0.1% des transactions).
*   Si on rate une fraude (Faux N√©gatif), la banque perd de l'argent.
*   Si on bloque une carte pour rien (Faux Positif), le client est juste un peu agac√©.

**Cas B : Filtre Anti-Spam**
*   Si on laisse passer un spam (Faux N√©gatif), c'est g√™nant.
*   Si on met un mail important (offre d'emploi) dans les spams (Faux Positif), c'est catastrophique.

**Questions :**
1.  Dans le Cas A (Fraude), doit-on privil√©gier la Pr√©cision ou le Rappel ? Pourquoi ?
2.  Dans le Cas B (Spam), doit-on privil√©gier la Pr√©cision ou le Rappel ? Pourquoi ?
3.  Un mod√®le qui pr√©dit "Pas Fraude" pour toutes les transactions aura une Accuracy de 99.9%. Est-il utile ? Qu'est-ce que cela nous apprend sur l'Accuracy ?
